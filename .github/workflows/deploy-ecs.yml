# yaml-language-server: $schema=https://json.schemastore.org/github-workflow
name: Deploy (Development -> Production)

on:
  # Auto-deploy to development when CI passes on main
  workflow_run:
    workflows: ["CI"]
    types: [completed]
    branches: [main]

  # Manual trigger for production or force redeploy
  workflow_dispatch:
    inputs:
      deploy_development:
        description: "Deploy development"
        required: true
        type: boolean
        default: true
      deploy_production:
        description: "Also deploy production (requires approval)"
        required: true
        type: boolean
        default: false
      force_redeploy:
        description: "Force a new deployment even if the commit is unchanged"
        required: true
        type: boolean
        default: false

env:
  AWS_REGION: us-east-1

jobs:
  deploy_development:
    name: Deploy development
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    environment:
      name: development
    concurrency:
      group: deploy-development
      cancel-in-progress: false
    # Run on: 1) successful CI on main, or 2) manual trigger with deploy_development=true
    if: |
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success') ||
      (github.event_name == 'workflow_dispatch' && inputs.deploy_development)

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # Use the triggering commit SHA for workflow_run events
          ref: ${{ github.event_name == 'workflow_run' && github.event.workflow_run.head_sha || github.sha }}

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.6.0"

      - name: Set image tag
        id: image_tag
        run: |
          # Use workflow_run SHA for auto-deploy, otherwise use current SHA
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            COMMIT_SHA="${{ github.event.workflow_run.head_sha }}"
          else
            COMMIT_SHA="${{ github.sha }}"
          fi
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ inputs.force_redeploy }}" = "true" ]; then
            echo "tag=${COMMIT_SHA}-${{ github.run_id }}" >> "$GITHUB_OUTPUT"
          else
            echo "tag=${COMMIT_SHA}" >> "$GITHUB_OUTPUT"
          fi

      - name: Terraform init
        working-directory: infra/terraform
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=medspa-ai-platform/development/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}"

      - name: Validate HTTPS certificate configured
        run: |
          if [ -z "${{ secrets.API_CERTIFICATE_ARN }}" ]; then
            echo "Missing API_CERTIFICATE_ARN secret (required to keep development HTTPS in Terraform)."
            exit 1
          fi
          if [ -z "${{ secrets.API_PUBLIC_BASE_URL }}" ]; then
            echo "Missing API_PUBLIC_BASE_URL secret (e.g., https://api-dev.example.com)."
            exit 1
          fi
          if [ -z "${{ secrets.UI_DOMAIN_NAME }}" ]; then
            echo "Missing UI_DOMAIN_NAME secret (e.g., portal-dev.aiwolfsolutions.com)."
            exit 1
          fi
          if [ -z "${{ secrets.ONBOARDING_COGNITO_USER_POOL_ID }}" ]; then
            echo "Missing ONBOARDING_COGNITO_USER_POOL_ID secret."
            exit 1
          fi
          if [ -z "${{ secrets.ONBOARDING_COGNITO_CLIENT_ID }}" ]; then
            echo "Missing ONBOARDING_COGNITO_CLIENT_ID secret."
            exit 1
          fi
          if [ -z "${{ secrets.ONBOARDING_COGNITO_REGION }}" ]; then
            echo "Missing ONBOARDING_COGNITO_REGION secret."
            exit 1
          fi

      - name: Terraform bootstrap (only if ECR repos missing)
        working-directory: infra/terraform
        run: |
          set -euo pipefail

          REGION="${{ env.AWS_REGION }}"
          ENV_NAME="development"

          NEED_BOOTSTRAP=0
          for REPO in "medspa-${ENV_NAME}-api" "medspa-${ENV_NAME}-voice-lambda" "medspa-${ENV_NAME}-browser-sidecar"; do
            if ! aws ecr describe-repositories --region "$REGION" --repository-names "$REPO" >/dev/null 2>&1; then
              NEED_BOOTSTRAP=1
              echo "ECR repo missing: $REPO"
            fi
          done

          if [ "$NEED_BOOTSTRAP" -eq 0 ]; then
            echo "All ECR repos exist; skipping bootstrap apply."
            exit 0
          fi

          echo "ECR repos missing; running Terraform bootstrap apply (voice webhooks disabled)."
          terraform apply -auto-approve \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="environment=development" \
            -var="api_image_tag=${{ steps.image_tag.outputs.tag }}" \
            -var="api_desired_count=0" \
            -var="enable_voice_webhooks=false" \
            -var="api_certificate_arn=${{ secrets.API_CERTIFICATE_ARN }}" \
            -var="api_public_base_url=${{ secrets.API_PUBLIC_BASE_URL }}" \
            -var="voice_upstream_base_url=${{ secrets.API_PUBLIC_BASE_URL }}" \
            -var="ui_domain_name=${{ secrets.UI_DOMAIN_NAME }}" \
            -var="ui_certificate_arn=${{ secrets.API_CERTIFICATE_ARN }}" \
            -var="cognito_user_pool_id=${{ secrets.ONBOARDING_COGNITO_USER_POOL_ID }}" \
            -var="cognito_client_id=${{ secrets.ONBOARDING_COGNITO_CLIENT_ID }}" \
            -var="cognito_region=${{ secrets.ONBOARDING_COGNITO_REGION }}" \
            -var="enable_blue_green=true" \
            -var="enable_browser_sidecar=true" \
            -var="assign_public_ip=true" \
            -var="enable_nat_gateway=false"

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region "${{ env.AWS_REGION }}" | \
            docker login --username AWS --password-stdin "${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"

      - name: Build & push API image
        env:
          IMAGE: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/medspa-development-api:${{ steps.image_tag.outputs.tag }}
        run: |
          docker build --target api -t "$IMAGE" .
          docker push "$IMAGE"

      - name: Build & push DB migrator image
        env:
          IMAGE: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/medspa-development-api:migrate-${{ steps.image_tag.outputs.tag }}
        run: |
          docker build --target migrate -t "$IMAGE" .
          docker push "$IMAGE"

      - name: Build & push voice-lambda image
        env:
          IMAGE: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/medspa-development-voice-lambda:${{ steps.image_tag.outputs.tag }}
        run: |
          docker build --target voice-lambda -t "$IMAGE" .
          docker push "$IMAGE"

      - name: Build & push browser-sidecar image
        env:
          IMAGE: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/medspa-development-browser-sidecar:${{ steps.image_tag.outputs.tag }}
        run: |
          docker build -t "$IMAGE" ./browser-sidecar
          docker push "$IMAGE"

      - name: Set up Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: web/onboarding/package-lock.json

      - name: Install onboarding dependencies
        working-directory: web/onboarding
        run: npm ci

      - name: Build onboarding UI
        working-directory: web/onboarding
        env:
          VITE_API_URL: ${{ secrets.API_PUBLIC_BASE_URL }}
          VITE_COGNITO_USER_POOL_ID: ${{ secrets.ONBOARDING_COGNITO_USER_POOL_ID }}
          VITE_COGNITO_CLIENT_ID: ${{ secrets.ONBOARDING_COGNITO_CLIENT_ID }}
          VITE_COGNITO_REGION: ${{ secrets.ONBOARDING_COGNITO_REGION }}
        run: npm run build

      - name: Terraform apply (full)
        working-directory: infra/terraform
        run: |
          terraform apply -auto-approve \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="environment=development" \
            -var="api_image_tag=${{ steps.image_tag.outputs.tag }}" \
            -var="api_certificate_arn=${{ secrets.API_CERTIFICATE_ARN }}" \
            -var="api_public_base_url=${{ secrets.API_PUBLIC_BASE_URL }}" \
            -var="voice_upstream_base_url=${{ secrets.API_PUBLIC_BASE_URL }}" \
            -var="ui_domain_name=${{ secrets.UI_DOMAIN_NAME }}" \
            -var="ui_certificate_arn=${{ secrets.API_CERTIFICATE_ARN }}" \
            -var="cognito_user_pool_id=${{ secrets.ONBOARDING_COGNITO_USER_POOL_ID }}" \
            -var="cognito_client_id=${{ secrets.ONBOARDING_COGNITO_CLIENT_ID }}" \
            -var="cognito_region=${{ secrets.ONBOARDING_COGNITO_REGION }}" \
            -var="enable_blue_green=true" \
            -var="enable_browser_sidecar=true" \
            -var="assign_public_ip=true" \
            -var="enable_nat_gateway=false"

      - name: Read UI outputs
        id: ui
        working-directory: infra/terraform
        run: |
          echo "bucket=$(terraform output -raw ui_bucket_name)" >> "$GITHUB_OUTPUT"
          echo "distribution=$(terraform output -raw ui_cloudfront_distribution_id)" >> "$GITHUB_OUTPUT"

      - name: Upload onboarding UI
        if: steps.ui.outputs.bucket != ''
        run: |
          aws s3 sync web/onboarding/dist "s3://${{ steps.ui.outputs.bucket }}" --delete --exclude "booking/*" --exclude "prospects/*"

      - name: Upload test booking page
        if: steps.ui.outputs.bucket != ''
        run: |
          aws s3 sync web/test-booking "s3://${{ steps.ui.outputs.bucket }}/booking" --delete

      - name: Invalidate onboarding UI cache
        if: steps.ui.outputs.distribution != ''
        run: |
          aws cloudfront create-invalidation --distribution-id "${{ steps.ui.outputs.distribution }}" --paths "/*"

      - name: Refresh DATABASE_URL (RDS managed secret)
        run: |
          set -euo pipefail

          REGION="${{ env.AWS_REGION }}"
          ENV_NAME="development"
          DB_ID="medspa-${ENV_NAME}-db"
          APP_SECRET_ID="medspa-${ENV_NAME}-app-secrets"

          MASTER_SECRET_ARN="$(aws rds describe-db-instances --region "$REGION" --db-instance-identifier "$DB_ID" --query 'DBInstances[0].MasterUserSecret.SecretArn' --output text)"
          if [ -z "$MASTER_SECRET_ARN" ] || [ "$MASTER_SECRET_ARN" = "None" ]; then
            echo "RDS master user secret ARN not found for ${DB_ID}"
            exit 1
          fi

          HOST="$(aws rds describe-db-instances --region "$REGION" --db-instance-identifier "$DB_ID" --query 'DBInstances[0].Endpoint.Address' --output text)"
          USERNAME="$(aws rds describe-db-instances --region "$REGION" --db-instance-identifier "$DB_ID" --query 'DBInstances[0].MasterUsername' --output text)"
          PASSWORD="$(aws secretsmanager get-secret-value --region "$REGION" --secret-id "$MASTER_SECRET_ARN" --query 'SecretString' --output text | python3 -c 'import json,sys; print(json.load(sys.stdin)["password"])')"

          ENCODED_PASSWORD="$(python3 -c 'import urllib.parse,sys; print(urllib.parse.quote(sys.argv[1], safe=""))' "$PASSWORD")"
          DB_URL="postgresql://${USERNAME}:${ENCODED_PASSWORD}@${HOST}:5432/medspa?sslmode=require"

          CURRENT="$(aws secretsmanager get-secret-value --region "$REGION" --secret-id "$APP_SECRET_ID" --query 'SecretString' --output text)"
          UPDATED="$(
            CURRENT="$CURRENT" DB_URL="$DB_URL" python3 - <<'PY'
          import json
          import os
          import sys

          # Guardrail: never overwrite the whole app secret if it's missing core keys.
          # If the secret was accidentally wiped, failing the deploy is safer than
          # persisting an empty payload with only DATABASE_URL set.
          current = json.loads(os.environ["CURRENT"])
          if not isinstance(current, dict):
            print("App secret is not a JSON object; refusing to update.", file=sys.stderr)
            sys.exit(1)

          must_have = ["ADMIN_JWT_SECRET", "TELNYX_API_KEY"]
          missing = [k for k in must_have if not current.get(k)]
          if missing:
            print(f"App secret missing required keys: {missing}; refusing to update.", file=sys.stderr)
            sys.exit(1)

          current["DATABASE_URL"] = os.environ["DB_URL"]
          print(json.dumps(current))
          PY
          )"

          aws secretsmanager put-secret-value --region "$REGION" --secret-id "$APP_SECRET_ID" --secret-string "$UPDATED" >/dev/null

      - name: Deploy API (blue/green via CodeDeploy) and wait
        working-directory: infra/terraform
        run: |
          set -euo pipefail

          APP="$(terraform output -raw codedeploy_app_name 2>/dev/null || true)"
          GROUP="$(terraform output -raw codedeploy_deployment_group_name 2>/dev/null || true)"
          TASK_DEF="$(terraform output -raw api_task_definition_arn 2>/dev/null || true)"

          if [ -n "$APP" ] && [ -n "$GROUP" ] && [ -n "$TASK_DEF" ]; then
            echo "Triggering CodeDeploy blue/green deployment: ${APP} / ${GROUP}"

            read -r CONTAINER_NAME CONTAINER_PORT <<< "$(aws ecs describe-task-definition --region "${{ env.AWS_REGION }}" --task-definition "$TASK_DEF" | python3 -c 'import json,sys; d=json.load(sys.stdin); c=d["taskDefinition"]["containerDefinitions"][0]; p=(c.get("portMappings") or [{"containerPort": 0}])[0]["containerPort"]; print(c["name"], p)')"

            if [ -z "${CONTAINER_PORT}" ] || [ "${CONTAINER_PORT}" = "0" ]; then
              echo "Failed to determine container port from task definition: ${TASK_DEF}"
              exit 1
            fi

            export APP GROUP TASK_DEF CONTAINER_NAME CONTAINER_PORT

            python3 - <<'PY'
          import hashlib
          import json
          import os

          task_def = os.environ["TASK_DEF"]
          container_name = os.environ["CONTAINER_NAME"]
          container_port = int(os.environ["CONTAINER_PORT"])

          appspec = (
              "version: 1\n"
              "Resources:\n"
              "  - TargetService:\n"
              "      Type: AWS::ECS::Service\n"
              "      Properties:\n"
              f"        TaskDefinition: \"{task_def}\"\n"
              "        LoadBalancerInfo:\n"
              f"          ContainerName: \"{container_name}\"\n"
              f"          ContainerPort: {container_port}\n"
          )

          payload = {
              "applicationName": os.environ["APP"],
              "deploymentGroupName": os.environ["GROUP"],
              "deploymentConfigName": "CodeDeployDefault.ECSAllAtOnce",
              "revision": {
                  "revisionType": "AppSpecContent",
                  "appSpecContent": {
                      "content": appspec,
                      "sha256": hashlib.sha256(appspec.encode("utf-8")).hexdigest(),
                  },
              },
          }

          with open("deployment.json", "w", encoding="utf-8") as f:
              json.dump(payload, f)
          PY

            DEPLOYMENT_ID="$(aws deploy create-deployment --region "${{ env.AWS_REGION }}" --cli-input-json file://deployment.json --query deploymentId --output text)"
            echo "DeploymentId=${DEPLOYMENT_ID}"
            aws deploy wait deployment-successful --region "${{ env.AWS_REGION }}" --deployment-id "${DEPLOYMENT_ID}"
          else
            echo "CodeDeploy blue/green disabled (missing outputs); waiting for ECS stability."
            CLUSTER="$(terraform output -raw ecs_cluster_name)"
            SERVICE="$(terraform output -raw ecs_service_name)"
            aws ecs wait services-stable --region "${{ env.AWS_REGION }}" --cluster "$CLUSTER" --services "$SERVICE"
          fi

  post_deploy_smoke_test:
    name: Post-deploy smoke test (dev)
    runs-on: ubuntu-latest
    needs: [deploy_development]
    environment:
      name: development
    if: |
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success') ||
      (github.event_name == 'workflow_dispatch' && inputs.deploy_development)

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event_name == 'workflow_run' && github.event.workflow_run.head_sha || github.sha }}

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version-file: go.mod

      - name: Wait for service to stabilize
        run: sleep 30

      - name: Run E2E smoke test (happy-path)
        env:
          API_BASE_URL: ${{ secrets.API_PUBLIC_BASE_URL }}
          ADMIN_JWT_SECRET: ${{ secrets.ADMIN_JWT_SECRET }}
        run: |
          go run scripts/e2e/run_e2e.go happy-path

  deploy_production:
    name: Deploy production (gated)
    runs-on: ubuntu-latest
    needs: [deploy_development]
    permissions:
      contents: read
      id-token: write
    environment:
      name: production
    concurrency:
      group: deploy-production
      cancel-in-progress: false
    if: github.event_name == 'workflow_dispatch' && inputs.deploy_production && github.ref_name == 'main'

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.6.0"

      - name: Set image tag
        id: image_tag
        run: |
          if [ "${{ inputs.force_redeploy }}" = "true" ]; then
            echo "tag=${{ github.sha }}-${{ github.run_id }}" >> "$GITHUB_OUTPUT"
          else
            echo "tag=${{ github.sha }}" >> "$GITHUB_OUTPUT"
          fi

      - name: Terraform init
        working-directory: infra/terraform
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=medspa-ai-platform/production/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}"

      - name: Validate HTTPS certificate configured
        run: |
          if [ -z "${{ secrets.API_CERTIFICATE_ARN }}" ]; then
            echo "Missing API_CERTIFICATE_ARN secret (required for production HTTPS)."
            exit 1
          fi
          if [ -z "${{ secrets.API_PUBLIC_BASE_URL }}" ]; then
            echo "Missing API_PUBLIC_BASE_URL secret (e.g., https://api.example.com)."
            exit 1
          fi
          if [ -z "${{ secrets.UI_DOMAIN_NAME }}" ]; then
            echo "Missing UI_DOMAIN_NAME secret (e.g., portal.aiwolfsolutions.com)."
            exit 1
          fi
          if [ -z "${{ secrets.ONBOARDING_COGNITO_USER_POOL_ID }}" ]; then
            echo "Missing ONBOARDING_COGNITO_USER_POOL_ID secret."
            exit 1
          fi
          if [ -z "${{ secrets.ONBOARDING_COGNITO_CLIENT_ID }}" ]; then
            echo "Missing ONBOARDING_COGNITO_CLIENT_ID secret."
            exit 1
          fi
          if [ -z "${{ secrets.ONBOARDING_COGNITO_REGION }}" ]; then
            echo "Missing ONBOARDING_COGNITO_REGION secret."
            exit 1
          fi

      - name: Terraform bootstrap (only if ECR repos missing)
        working-directory: infra/terraform
        run: |
          set -euo pipefail

          REGION="${{ env.AWS_REGION }}"
          ENV_NAME="production"

          NEED_BOOTSTRAP=0
          for REPO in "medspa-${ENV_NAME}-api" "medspa-${ENV_NAME}-voice-lambda" "medspa-${ENV_NAME}-browser-sidecar"; do
            if ! aws ecr describe-repositories --region "$REGION" --repository-names "$REPO" >/dev/null 2>&1; then
              NEED_BOOTSTRAP=1
              echo "ECR repo missing: $REPO"
            fi
          done

          if [ "$NEED_BOOTSTRAP" -eq 0 ]; then
            echo "All ECR repos exist; skipping bootstrap apply."
            exit 0
          fi

          echo "ECR repos missing; running Terraform bootstrap apply (voice webhooks disabled)."
          terraform apply -auto-approve \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="environment=production" \
            -var="api_image_tag=${{ steps.image_tag.outputs.tag }}" \
            -var="api_desired_count=0" \
            -var="enable_voice_webhooks=false" \
            -var="api_certificate_arn=${{ secrets.API_CERTIFICATE_ARN }}" \
            -var="api_public_base_url=${{ secrets.API_PUBLIC_BASE_URL }}" \
            -var="voice_upstream_base_url=${{ secrets.API_PUBLIC_BASE_URL }}" \
            -var="ui_domain_name=${{ secrets.UI_DOMAIN_NAME }}" \
            -var="ui_certificate_arn=${{ secrets.API_CERTIFICATE_ARN }}" \
            -var="cognito_user_pool_id=${{ secrets.ONBOARDING_COGNITO_USER_POOL_ID }}" \
            -var="cognito_client_id=${{ secrets.ONBOARDING_COGNITO_CLIENT_ID }}" \
            -var="cognito_region=${{ secrets.ONBOARDING_COGNITO_REGION }}" \
            -var="enable_blue_green=true" \
            -var="enable_browser_sidecar=true"

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region "${{ env.AWS_REGION }}" | \
            docker login --username AWS --password-stdin "${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"

      - name: Build & push API image
        env:
          IMAGE: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/medspa-production-api:${{ steps.image_tag.outputs.tag }}
        run: |
          docker build --target api -t "$IMAGE" .
          docker push "$IMAGE"

      - name: Build & push DB migrator image
        env:
          IMAGE: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/medspa-production-api:migrate-${{ steps.image_tag.outputs.tag }}
        run: |
          docker build --target migrate -t "$IMAGE" .
          docker push "$IMAGE"

      - name: Build & push voice-lambda image
        env:
          IMAGE: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/medspa-production-voice-lambda:${{ steps.image_tag.outputs.tag }}
        run: |
          docker build --target voice-lambda -t "$IMAGE" .
          docker push "$IMAGE"

      - name: Build & push browser-sidecar image
        env:
          IMAGE: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/medspa-production-browser-sidecar:${{ steps.image_tag.outputs.tag }}
        run: |
          docker build -t "$IMAGE" ./browser-sidecar
          docker push "$IMAGE"

      - name: Set up Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: web/onboarding/package-lock.json

      - name: Install onboarding dependencies
        working-directory: web/onboarding
        run: npm ci

      - name: Build onboarding UI
        working-directory: web/onboarding
        env:
          VITE_API_URL: ${{ secrets.API_PUBLIC_BASE_URL }}
          VITE_COGNITO_USER_POOL_ID: ${{ secrets.ONBOARDING_COGNITO_USER_POOL_ID }}
          VITE_COGNITO_CLIENT_ID: ${{ secrets.ONBOARDING_COGNITO_CLIENT_ID }}
          VITE_COGNITO_REGION: ${{ secrets.ONBOARDING_COGNITO_REGION }}
        run: npm run build

      - name: Terraform apply (full)
        working-directory: infra/terraform
        run: |
          terraform apply -auto-approve \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="environment=production" \
            -var="api_image_tag=${{ steps.image_tag.outputs.tag }}" \
            -var="api_certificate_arn=${{ secrets.API_CERTIFICATE_ARN }}" \
            -var="api_public_base_url=${{ secrets.API_PUBLIC_BASE_URL }}" \
            -var="voice_upstream_base_url=${{ secrets.API_PUBLIC_BASE_URL }}" \
            -var="ui_domain_name=${{ secrets.UI_DOMAIN_NAME }}" \
            -var="ui_certificate_arn=${{ secrets.API_CERTIFICATE_ARN }}" \
            -var="cognito_user_pool_id=${{ secrets.ONBOARDING_COGNITO_USER_POOL_ID }}" \
            -var="cognito_client_id=${{ secrets.ONBOARDING_COGNITO_CLIENT_ID }}" \
            -var="cognito_region=${{ secrets.ONBOARDING_COGNITO_REGION }}" \
            -var="enable_blue_green=true" \
            -var="enable_browser_sidecar=true"

      - name: Read UI outputs
        id: ui
        working-directory: infra/terraform
        run: |
          echo "bucket=$(terraform output -raw ui_bucket_name)" >> "$GITHUB_OUTPUT"
          echo "distribution=$(terraform output -raw ui_cloudfront_distribution_id)" >> "$GITHUB_OUTPUT"

      - name: Upload onboarding UI
        if: steps.ui.outputs.bucket != ''
        run: |
          aws s3 sync web/onboarding/dist "s3://${{ steps.ui.outputs.bucket }}" --delete --exclude "booking/*" --exclude "prospects/*"

      - name: Upload test booking page
        if: steps.ui.outputs.bucket != ''
        run: |
          aws s3 sync web/test-booking "s3://${{ steps.ui.outputs.bucket }}/booking" --delete

      - name: Invalidate onboarding UI cache
        if: steps.ui.outputs.distribution != ''
        run: |
          aws cloudfront create-invalidation --distribution-id "${{ steps.ui.outputs.distribution }}" --paths "/*"

      - name: Refresh DATABASE_URL (RDS managed secret)
        run: |
          set -euo pipefail

          REGION="${{ env.AWS_REGION }}"
          ENV_NAME="production"
          DB_ID="medspa-${ENV_NAME}-db"
          APP_SECRET_ID="medspa-${ENV_NAME}-app-secrets"

          MASTER_SECRET_ARN="$(aws rds describe-db-instances --region "$REGION" --db-instance-identifier "$DB_ID" --query 'DBInstances[0].MasterUserSecret.SecretArn' --output text)"
          if [ -z "$MASTER_SECRET_ARN" ] || [ "$MASTER_SECRET_ARN" = "None" ]; then
            echo "RDS master user secret ARN not found for ${DB_ID}"
            exit 1
          fi

          HOST="$(aws rds describe-db-instances --region "$REGION" --db-instance-identifier "$DB_ID" --query 'DBInstances[0].Endpoint.Address' --output text)"
          USERNAME="$(aws rds describe-db-instances --region "$REGION" --db-instance-identifier "$DB_ID" --query 'DBInstances[0].MasterUsername' --output text)"
          PASSWORD="$(aws secretsmanager get-secret-value --region "$REGION" --secret-id "$MASTER_SECRET_ARN" --query 'SecretString' --output text | python3 -c 'import json,sys; print(json.load(sys.stdin)["password"])')"

          ENCODED_PASSWORD="$(python3 -c 'import urllib.parse,sys; print(urllib.parse.quote(sys.argv[1], safe=""))' "$PASSWORD")"
          DB_URL="postgresql://${USERNAME}:${ENCODED_PASSWORD}@${HOST}:5432/medspa?sslmode=require"

          CURRENT="$(aws secretsmanager get-secret-value --region "$REGION" --secret-id "$APP_SECRET_ID" --query 'SecretString' --output text)"
          UPDATED="$(
            CURRENT="$CURRENT" DB_URL="$DB_URL" python3 - <<'PY'
          import json
          import os
          import sys

          # Guardrail: never overwrite the whole app secret if it's missing core keys.
          # If the secret was accidentally wiped, failing the deploy is safer than
          # persisting an empty payload with only DATABASE_URL set.
          current = json.loads(os.environ["CURRENT"])
          if not isinstance(current, dict):
            print("App secret is not a JSON object; refusing to update.", file=sys.stderr)
            sys.exit(1)

          must_have = ["ADMIN_JWT_SECRET", "TELNYX_API_KEY"]
          missing = [k for k in must_have if not current.get(k)]
          if missing:
            print(f"App secret missing required keys: {missing}; refusing to update.", file=sys.stderr)
            sys.exit(1)

          current["DATABASE_URL"] = os.environ["DB_URL"]
          print(json.dumps(current))
          PY
          )"

          aws secretsmanager put-secret-value --region "$REGION" --secret-id "$APP_SECRET_ID" --secret-string "$UPDATED" >/dev/null

      - name: Deploy API (blue/green via CodeDeploy) and wait
        working-directory: infra/terraform
        run: |
          set -euo pipefail

          APP="$(terraform output -raw codedeploy_app_name 2>/dev/null || true)"
          GROUP="$(terraform output -raw codedeploy_deployment_group_name 2>/dev/null || true)"
          TASK_DEF="$(terraform output -raw api_task_definition_arn 2>/dev/null || true)"

          if [ -n "$APP" ] && [ -n "$GROUP" ] && [ -n "$TASK_DEF" ]; then
            echo "Triggering CodeDeploy blue/green deployment: ${APP} / ${GROUP}"

            read -r CONTAINER_NAME CONTAINER_PORT <<< "$(aws ecs describe-task-definition --region "${{ env.AWS_REGION }}" --task-definition "$TASK_DEF" | python3 -c 'import json,sys; d=json.load(sys.stdin); c=d["taskDefinition"]["containerDefinitions"][0]; p=(c.get("portMappings") or [{"containerPort": 0}])[0]["containerPort"]; print(c["name"], p)')"

            if [ -z "${CONTAINER_PORT}" ] || [ "${CONTAINER_PORT}" = "0" ]; then
              echo "Failed to determine container port from task definition: ${TASK_DEF}"
              exit 1
            fi

            export APP GROUP TASK_DEF CONTAINER_NAME CONTAINER_PORT

            python3 - <<'PY'
          import hashlib
          import json
          import os

          task_def = os.environ["TASK_DEF"]
          container_name = os.environ["CONTAINER_NAME"]
          container_port = int(os.environ["CONTAINER_PORT"])

          appspec = (
              "version: 1\n"
              "Resources:\n"
              "  - TargetService:\n"
              "      Type: AWS::ECS::Service\n"
              "      Properties:\n"
              f"        TaskDefinition: \"{task_def}\"\n"
              "        LoadBalancerInfo:\n"
              f"          ContainerName: \"{container_name}\"\n"
              f"          ContainerPort: {container_port}\n"
          )

          payload = {
              "applicationName": os.environ["APP"],
              "deploymentGroupName": os.environ["GROUP"],
              "deploymentConfigName": "CodeDeployDefault.ECSAllAtOnce",
              "revision": {
                  "revisionType": "AppSpecContent",
                  "appSpecContent": {
                      "content": appspec,
                      "sha256": hashlib.sha256(appspec.encode("utf-8")).hexdigest(),
                  },
              },
          }

          with open("deployment.json", "w", encoding="utf-8") as f:
              json.dump(payload, f)
          PY

            DEPLOYMENT_ID="$(aws deploy create-deployment --region "${{ env.AWS_REGION }}" --cli-input-json file://deployment.json --query deploymentId --output text)"
            echo "DeploymentId=${DEPLOYMENT_ID}"
            aws deploy wait deployment-successful --region "${{ env.AWS_REGION }}" --deployment-id "${DEPLOYMENT_ID}"
          else
            echo "CodeDeploy blue/green disabled (missing outputs); waiting for ECS stability."
            CLUSTER="$(terraform output -raw ecs_cluster_name)"
            SERVICE="$(terraform output -raw ecs_service_name)"
            aws ecs wait services-stable --region "${{ env.AWS_REGION }}" --cluster "$CLUSTER" --services "$SERVICE"
          fi
